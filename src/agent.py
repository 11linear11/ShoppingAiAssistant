"""
Shopping AI Assistant using LangGraph
A conversational agent that can search for products using Elasticsearch and semantic search.
"""

from dotenv import load_dotenv
load_dotenv()

import os
import logging
from typing import Annotated
from typing_extensions import TypedDict

from langchain_nvidia_ai_endpoints import ChatNVIDIA
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, SystemMessage
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode ,tools_condition
from langgraph.checkpoint.memory import MemorySaver

# Import the semantic search tool
from .tools.SearchProducts import search_products_semantic , interpret_query


# Configuration
api_key = os.getenv("api_key")
BASE_URL = "https://integrate.api.nvidia.com/v1"

# api_key = os.getenv("OPENAI_API_KEY")
# BASE_URL = "https://models.inference.ai.azure.com"
DEBUG_MODE = os.getenv("DEBUG_MODE", "false").lower() == "true"

# Setup logging
def setup_logging():
    """Configure logging based on DEBUG_MODE."""
    if DEBUG_MODE:
        logging.basicConfig(
            level=logging.DEBUG,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('shopping_assistant_debug.log'),
                logging.StreamHandler()
            ]
        )
    else:
        logging.basicConfig(
            level=logging.WARNING,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
    
    return logging.getLogger(__name__)

logger = setup_logging()

# System prompt
SYSTEM_PROMPT = """You are an intelligent shopping assistant. Your task is that when the user intends to find a product, you analyze their phrase and then perform a product search.

You only have two tools:

1) interpret_query  
   Input: {"query": "<full user text>"}  
   Output: includes information such as category, intent, price_sensitivity, quality_sensitivity, AND suggested_query

2) search_products_semantic  
   Input: {"query": "<product keyword>", "quality_sensitivity": 0.5, "price_sensitivity": 0.5, "category": "<category>", "intent": "<intent>"}  
   You must use ALL outputs from interpret_query including category, intent, AND suggested_query.

-----------------------------------------------
### Mandatory Rules
- If the user intends to buy or search for a product, you **must** use both tools.
- Always call interpret_query first.
- Then based on its output, call search_products_semantic and **you must pass ALL these fields**:
  * query: USE "suggested_query" from interpret_query output! (This is the most important field)
  * category: from interpret_query output (if not null)
  * intent: from interpret_query output (ALWAYS pass this!)
  * price_sensitivity: from interpret_query output
  * quality_sensitivity: from interpret_query output
  
- Example flow 1 (direct product mention):
  1. User says: "ÛŒÙ‡ Ù‡Ø¯ÙÙˆÙ† Ø§Ø±Ø²Ø§Ù† Ù…ÛŒØ®ÙˆØ§Ù…"
  2. Call interpret_query({"query": "ÛŒÙ‡ Ù‡Ø¯ÙÙˆÙ† Ø§Ø±Ø²Ø§Ù† Ù…ÛŒØ®ÙˆØ§Ù…"})
  3. Get result: {"category": "Ù„ÙˆØ§Ø²Ù… Ø§Ù„Ú©ØªØ±ÙˆÙ†ÛŒÚ©ÛŒ", "intent": "find_cheapest", "price_sensitivity": 1.0, "quality_sensitivity": 0.0, "suggested_query": "Ù‡Ø¯ÙÙˆÙ†"}
  4. Call search_products_semantic({
       "query": "Ù‡Ø¯ÙÙˆÙ†",  â† Ø§Ø² suggested_query
       "category": "Ù„ÙˆØ§Ø²Ù… Ø§Ù„Ú©ØªØ±ÙˆÙ†ÛŒÚ©ÛŒ",
       "intent": "find_cheapest",
       "price_sensitivity": 1.0,
       "quality_sensitivity": 0.0
     })

- Example flow 2 (implicit intent - user describes need):
  1. User says: "ÛŒÚ†ÛŒØ² Ù…ÛŒØ®ÙˆØ§Ù… Ø¨Ù¾ÙˆØ´Ù… Ø³Ø±Ø¯Ù… Ù†Ø´Ù‡"
  2. Call interpret_query({"query": "ÛŒÚ†ÛŒØ² Ù…ÛŒØ®ÙˆØ§Ù… Ø¨Ù¾ÙˆØ´Ù… Ø³Ø±Ø¯Ù… Ù†Ø´Ù‡"})
  3. Get result: {"category": "Ù…Ø¯ Ùˆ Ù¾ÙˆØ´Ø§Ú©", "intent": "find_by_feature", "price_sensitivity": 0.5, "quality_sensitivity": 0.5, "suggested_query": "Ú©Ø§Ù¾Ø´Ù†"}
  4. Call search_products_semantic({
       "query": "Ú©Ø§Ù¾Ø´Ù†",  â† Ø§Ø² suggested_query (Ù†Ù‡ Ù…ØªÙ† Ú©Ø§Ø±Ø¨Ø±!)
       "category": "Ù…Ø¯ Ùˆ Ù¾ÙˆØ´Ø§Ú©",
       "intent": "find_by_feature",
       "price_sensitivity": 0.5,
       "quality_sensitivity": 0.5
     })

- The final output must only be the result of search_products_semantic in an organized and structured format.

-----------------------------------------------
### Output Format Rules (VERY IMPORTANT!)

When you receive products from search_products_semantic, you MUST:

1. **Check Relevance First:**
   - Compare the user's original query with the found products
   - If products don't match the user's intent, say so clearly
   - Example: User asked for "Ù‡Ø¯ÙÙˆÙ†" but got "Ú©Ø§Ø¨Ù„ Ø´Ø§Ø±Ú˜" â†’ Tell user no relevant products found

2. **Show Min/Max Products:**
   - Minimum: Show at least 1 product (the best match)
   - Maximum: Show at most 5 products
   - If more than 5, show top 5 by value_score

3. **Format Each Product in Persian:**
```
ğŸ›’ [Ù†Ø§Ù… Ù…Ø­ØµÙˆÙ„]
   ğŸ’° Ù‚ÛŒÙ…Øª: [final_price] ØªÙˆÙ…Ø§Ù†
   ğŸ·ï¸ Ø¨Ø±Ù†Ø¯: [brand]
   ğŸ”¥ ØªØ®ÙÛŒÙ: [discount]%
```

4. **Add Summary at End:**
```
---
ğŸ“Š Ø®Ù„Ø§ØµÙ‡: [ØªØ¹Ø¯Ø§Ø¯] Ù…Ø­ØµÙˆÙ„ ÛŒØ§ÙØª Ø´Ø¯ | Ø¨Ø§Ø²Ù‡ Ù‚ÛŒÙ…Øª: [min] - [max] ØªÙˆÙ…Ø§Ù†
```

5. **Relevance Check Response:**
   If products are NOT relevant to query:
```
Ù…ØªØ£Ø³ÙØ§Ù†Ù‡ Ù…Ø­ØµÙˆÙ„ÛŒ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ "[query Ú©Ø§Ø±Ø¨Ø±]" Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.
Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯: [ÛŒÚ© Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù…Ø±ØªØ¨Ø·]
```


-----------------------------------------------
### Intent meanings (for your reference):
- find_cheapest: User wants the cheapest option â†’ results sorted by lowest price
- find_high_quality: User wants best quality â†’ results sorted by brand score
- find_best_value: User wants best price/quality ratio â†’ balanced results
- find_by_feature: User mentioned specific feature â†’ results prioritize similarity
- compare: User wants to compare options â†’ more diverse results shown

-----------------------------------------------
### Detecting whether the user intends to search for a product:
If the user's text contains any of the following, the user intends to buy:
- Action words: "Ù¾ÛŒØ¯Ø§ Ú©Ù†", "Ù…ÛŒâ€ŒØ®ÙˆØ§Ù…", "Ø¬Ø³ØªØ¬Ùˆ", "Ø¨Ú¯Ø±Ø¯", "Ù†Ø´ÙˆÙ† Ø¨Ø¯Ù‡", "Ù…Ø¹Ø±ÙÛŒ Ú©Ù†"
- Or contains the name of a product: Ø¯ÙˆØºØŒ Ø´ÙˆØ±ØªØŒ Ú¯ÙˆØ´ÛŒØŒ Ù„Ù¾ØªØ§Ù¾ØŒ Ú©ÙØ´ØŒ Ù…Ø§Ù†ÛŒØªÙˆØ±ØŒ Ù‡Ù†Ø¯Ø²ÙØ±ÛŒ etc.
- Or describes a NEED: "Ú¯Ø´Ù†Ù…Ù‡", "ØªØ´Ù†Ù…Ù‡", "Ø³Ø±Ø¯Ù…Ù‡", "Ø®ÙˆØ§Ø¨Ù… Ù…ÛŒØ§Ø¯", "Ù¾ÙˆØ³ØªÙ… Ø®Ø´Ú©Ù‡"

In this case:  
â‡’ You must call the tools.

If the user asks a general question, greeting, or non-shopping topic:  
â‡’ Do not use the tools and only respond with:  
{"message": "<your response>"}

-----------------------------------------------
### Non-shopping examples

User: "Ú†Ø·ÙˆØ±ÛŒØŸ"  
Response:  
{"message": "I'm good, how about you?"}

User: "Ø¯Ø§Ø³ØªØ§Ù† Ø§Ù†Ú¯ÛŒØ²Ø´ÛŒ Ø¨Ú¯Ùˆ"  
Response:  
{"message": "Sure..."}
-----------------------------------------------

### Important Note
- Do not produce any text outside these two modes.
- In normal messages, only return {"message": "..."}.
- ALWAYS use suggested_query from interpret_query as the query for search_products_semantic!


"""


class State(TypedDict):
    """State definition for the conversation graph."""
    messages: Annotated[list, add_messages]


def create_agent():
    """
    Create and configure the LangGraph agent with Elasticsearch-based product search.
    
    Returns:
        Compiled graph with memory
    """
    logger.info("ğŸš€ Creating Shopping AI Agent...")
    logger.debug(f"Debug Mode: {DEBUG_MODE}")
    logger.debug(f"LLM Model: openai/gpt-oss-20b")
    
    # Initialize LLM
    llm = ChatNVIDIA(
        model="openai/gpt-oss-120b",
        api_key=api_key,
        base_url=BASE_URL,
        max_tokens=2048,  # Ø§ÙØ²Ø§ÛŒØ´ max_tokens Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² cut off
        temperature=0.3,  # Ú©Ù…ØªØ± Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ±
    )
    # llm = ChatOpenAI(
    #     model="gpt-4o",
    #     openai_api_key=api_key,
    #     openai_api_base=BASE_URL
    # )

    
    logger.debug("âœ… LLM initialized successfully")
    
    # Bind tools to LLM
    tools = [search_products_semantic, interpret_query]
    llm_with_tools = llm.bind_tools(tools)
    logger.debug(f"ğŸ”§ Tools bound: {[tool.name for tool in tools]}")
    
    # Define chatbot node
    def chatbot_node(state):
        """Process messages and generate responses."""
        logger.debug("=" * 60)
        logger.debug("ğŸ“¥ CHATBOT NODE - Processing messages")
        
        # Add system message if not already present
        messages = state["messages"]
        if not messages or not isinstance(messages[0], SystemMessage):
            messages = [SystemMessage(content=SYSTEM_PROMPT)] + messages
            logger.debug("ğŸ“ System prompt added to messages")
        
        logger.debug(f"ğŸ’¬ Message count: {len(messages)}")
        if messages:
            last_msg = messages[-1]
            logger.debug(f"ğŸ“¨ Last message type: {type(last_msg).__name__}")
            if hasattr(last_msg, 'content'):
                content_preview = str(last_msg.content)[:100]
                logger.debug(f"ğŸ“„ Content preview: {content_preview}...")
        
        logger.debug("ğŸ¤– Invoking LLM...")
        response = llm_with_tools.invoke(messages)
        
        logger.debug(f"âœ… LLM Response received")
        logger.debug(f"ğŸ“Š Response type: {type(response).__name__}")
        
        # Check if tools are being called
        if hasattr(response, 'tool_calls') and response.tool_calls:
            logger.info(f"ğŸ”§ Tool calls requested: {len(response.tool_calls)}")
            for i, tool_call in enumerate(response.tool_calls):
                logger.debug(f"  Tool {i+1}: {tool_call.get('name', 'unknown')}")
                logger.debug(f"  Args: {tool_call.get('args', {})}")
        else:
            logger.debug("ğŸ’¬ Direct response (no tool calls)")
            if hasattr(response, 'content'):
                content = str(response.content)
                logger.info(f"ğŸ“„ Response content length: {len(content)} chars")
                # Ù„Ø§Ú¯ Ú©Ø§Ù…Ù„ Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØ¨Ø§Ú¯
                if content:
                    logger.debug(f"ğŸ“„ Full response:\n{content}")
                else:
                    logger.warning("âš ï¸ LLM returned EMPTY content!")
        
        return {"messages": [response]}
    
    # Create tool node
    tool_node = ToolNode(tools)
    logger.debug("ğŸ› ï¸ Tool node created")
    
    
    
    # Build graph
    logger.debug("ğŸ—ï¸ Building graph structure...")
    builder = StateGraph(State)
    builder.add_node("chatbot", chatbot_node)
    builder.add_node("tools", tool_node)
    
    # Define edges
    builder.add_edge(START, "chatbot")
    builder.add_conditional_edges("chatbot", tools_condition)
    builder.add_edge("tools","chatbot")
    
    logger.debug("ğŸ”— Graph edges configured")
    
    # Compile with memory
    memory = MemorySaver()
    graph = builder.compile(checkpointer=memory)
    
    logger.info("âœ… Agent compiled successfully with memory")
    logger.debug("=" * 60)
    
    return graph
