"""
MCP Server for Query Interpretation
Port: 5004

This server handles:
- interpret_query: Analyze user shopping intent and prepare structured output for EQuIP DSL generation

New Architecture:
- Generates English query for EQuIP 3B model
- Provides token_mapping for Englishâ†’Persian translation in DSL
- Keeps Persian full query for BM25 and semantic search
"""

import os
import sys
import json
import re
import logging
import asyncio
from typing import Dict, Any, List
from contextlib import asynccontextmanager
from collections.abc import AsyncIterator

from dotenv import load_dotenv
from langchain_nvidia_ai_endpoints import ChatNVIDIA
from mcp.server.fastmcp import FastMCP
from mcp import ClientSession
from mcp.client.streamable_http import streamablehttp_client

load_dotenv()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Configuration
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SERVER_NAME = "interpret-server"
SERVER_PORT = 5004
EMBEDDING_SERVER_URL = "http://localhost:5003"
DEBUG_MODE = os.getenv("DEBUG_MODE", "false").lower() == "true"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Logging Setup
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
logging.basicConfig(
    level=logging.DEBUG if DEBUG_MODE else logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(SERVER_NAME)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LLM Service Class
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
class InterpretService:
    """Handles query interpretation using LLM for EQuIP DSL generation."""
    
    def __init__(self):
        logger.info("ğŸ”§ Initializing InterpretService...")
        
        # Initialize LLM - Using NVIDIA NIM
        nvidia_models = [
            "meta/llama-3.1-70b-instruct",
            "meta/llama-3.1-8b-instruct",
        ]
        model = os.getenv("NVIDIA_MODEL", nvidia_models[0])
        logger.info(f"ğŸ¤– Using NVIDIA model: {model}")
        
        self.llm = ChatNVIDIA(
            model=model,
            api_key=os.getenv("api_key"),
            base_url="https://integrate.api.nvidia.com/v1",
            temperature=0.1,
            max_tokens=2000
        )
        
        logger.info("âœ… InterpretService initialized")
    
    async def call_embedding_tool(self, tool_name: str, arguments: Dict) -> Dict:
        """Call the embedding server's MCP tool."""
        try:
            async with streamablehttp_client(f"{EMBEDDING_SERVER_URL}/") as (read, write, _):
                async with ClientSession(read, write) as session:
                    await session.initialize()
                    result = await session.call_tool(tool_name, arguments)
                    if result.content:
                        return json.loads(result.content[0].text)
                    return {"success": False, "error": "No content"}
        except Exception as e:
            logger.error(f"âŒ Error calling embedding server: {e}")
            return {"success": False, "error": str(e)}
    
    async def classify_categories(self, query: str, top_k: int = 3) -> list:
        """Call embedding server to classify categories."""
        try:
            result = await self.call_embedding_tool("classify_categories", {
                "query": query,
                "top_k": top_k
            })
            if result.get("success"):
                return result.get("categories", [])
            return []
        except Exception as e:
            logger.error(f"âŒ Error classifying categories: {e}")
            return []
    
    async def interpret(self, query: str) -> Dict[str, Any]:
        """
        Analyze user shopping intent and prepare structured output for EQuIP.
        
        Args:
            query: User's shopping query in Persian
            
        Returns:
            Dict with:
            - equip_prompt: English natural language query for EQuIP
            - persian_full_query: Full Persian product keywords for search
            - token_mapping: English -> Persian word mapping for DSL translation
            - categories_fa: Persian category names
            - intent: Shopping intent
            - price_sensitivity: 0-1
            - quality_sensitivity: 0-1
        """
        logger.info(f"ğŸ§  Interpreting query: '{query}'")
        
        # Default values
        equip_prompt = ""
        persian_full_query = query
        token_mapping = {}
        intent = "find_best_value"
        price_sens = 0.5
        quality_sens = 0.5
        
        # Build prompt for LLM
        # NOTE: Categories are NOT determined by LLM - they come from classify_categories after this step
        prompt = f"""You are a bilingual shopping query interpreter (Persian â†’ English).

Your task is to analyze the user's Persian shopping query and create:
1. A STRUCTURED English query for EQuIP (Elasticsearch DSL generator)
2. Extract Persian product keywords that should remain in Persian
3. Create a mapping between English words and their Persian equivalents

IMPORTANT RULES:
- The dataset has Persian product names like: "Ø´ÙˆØ±Øª ØµÙˆØ±ØªÛŒ Ù…Ø±Ø¯Ø§Ù†Ù‡ xl", "Ø´ÛŒØ± Ú©Ù… Ú†Ø±Ø¨ Ú©Ø§Ù„Ù‡"
- Product names contain all features together (color, size, type, brand)
- You must keep the FULL Persian product description for search
- DO NOT include category in equip_prompt - categories are determined separately

Output ONLY valid JSON in this EXACT format:
{{
    "equip_prompt": "product_name: <english product name with features> sort: <sort field> filter: <optional filters>",
    "persian_full_query": "Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ ÙØ§Ø±Ø³ÛŒ Ù…Ø­ØµÙˆÙ„ Ø¨Ø§ ØªÙ…Ø§Ù… ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§",
    "token_mapping": {{
        "english_word": "Ù…Ø¹Ø§Ø¯Ù„_ÙØ§Ø±Ø³ÛŒ"
    }},
    "intent": "find_cheapest|find_best_value|find_high_quality|find_by_feature|compare",
    "price_sensitivity": 0.0-1.0,
    "quality_sensitivity": 0.0-1.0
}}

### equip_prompt Structure:
- product_name: English product name with all attributes (color, size, brand, type)
- sort: price_asc, price_desc, relevance, quality (based on user intent)
- filter: optional filters like brand, size, color (only if explicitly mentioned)

### Intent Types:
- find_cheapest: user wants the cheapest option ("Ø§Ø±Ø²Ø§Ù†", "Ø§Ø±Ø²Ø§Ù†â€ŒØªØ±ÛŒÙ†") â†’ sort: price_asc
- find_best_value: user wants best price/quality ratio ("Ù…Ù‚Ø±ÙˆÙ†â€ŒØ¨Ù‡â€ŒØµØ±ÙÙ‡") â†’ sort: relevance
- find_high_quality: user prioritizes quality ("Ú©ÛŒÙÛŒØª Ø¨Ø§Ù„Ø§", "Ù…Ø­Ú©Ù…") â†’ sort: quality
- compare: user wants to compare options ("Ù…Ù‚Ø§ÛŒØ³Ù‡") â†’ sort: relevance
- find_by_feature: user mentions specific features (color, size, etc.) â†’ sort: relevance

### Price Sensitivity:
- 1.0: words like "Ø§Ø±Ø²ÙˆÙ†", "Ø§Ø±Ø²Ø§Ù†â€ŒØªØ±ÛŒÙ†", "Ù…Ù‚Ø±ÙˆÙ†â€ŒØ¨Ù‡â€ŒØµØ±ÙÙ‡"
- 0.5: indirect or unclear
- 0.0: no price-related mention

### Quality Sensitivity:
- 1.0: words like "Ú©ÛŒÙÛŒØª Ø¨Ø§Ù„Ø§", "Ù…Ø­Ú©Ù…", "Ù…Ø§Ø±Ú©â€ŒØ¯Ø§Ø±"
- 0.5: unclear
- 0.0: no quality mention

### Examples:

Input: "Ø´ÙˆØ±Øª ØµÙˆØ±ØªÛŒ Ù…Ø±Ø¯Ø§Ù†Ù‡ xl Ù…ÛŒØ®ÙˆØ§Ù… Ø§Ø±Ø²ÙˆÙ†"
Output: {{
    "equip_prompt": "product_name: pink men shorts xl sort: price_asc",
    "persian_full_query": "Ø´ÙˆØ±Øª ØµÙˆØ±ØªÛŒ Ù…Ø±Ø¯Ø§Ù†Ù‡ xl",
    "token_mapping": {{
        "pink": "ØµÙˆØ±ØªÛŒ",
        "men": "Ù…Ø±Ø¯Ø§Ù†Ù‡",
        "shorts": "Ø´ÙˆØ±Øª",
        "xl": "xl"
    }},
    "intent": "find_cheapest",
    "price_sensitivity": 1.0,
    "quality_sensitivity": 0.0
}}

Input: "Ø´ÛŒØ± Ø§Ø±Ø²ÙˆÙ† Ù…ÛŒØ®ÙˆØ§Ù…"
Output: {{
    "equip_prompt": "product_name: milk sort: price_asc",
    "persian_full_query": "Ø´ÛŒØ±",
    "token_mapping": {{
        "milk": "Ø´ÛŒØ±"
    }},
    "intent": "find_cheapest",
    "price_sensitivity": 1.0,
    "quality_sensitivity": 0.0
}}

Input: "Ù…Ù† Ø³Ø±Ø¯Ù…Ù‡"
Output: {{
    "equip_prompt": "product_name: jacket coat sweater warm clothing sort: relevance",
    "persian_full_query": "Ú©Ø§Ù¾Ø´Ù† Ú˜Ø§Ú©Øª Ù¾Ø§Ù„ØªÙˆ",
    "token_mapping": {{
        "jacket": "Ú©Ø§Ù¾Ø´Ù†",
        "coat": "Ù¾Ø§Ù„ØªÙˆ",
        "sweater": "Ú˜Ø§Ú©Øª"
    }},
    "intent": "find_by_feature",
    "price_sensitivity": 0.5,
    "quality_sensitivity": 0.5
}}

Input: "Ù‡Ø¯ÙÙˆÙ† Ø³ÙˆÙ†ÛŒ Ø¨Ø§Ú©ÛŒÙÛŒØª"
Output: {{
    "equip_prompt": "product_name: Sony headphones sort: quality filter: brand=Sony",
    "persian_full_query": "Ù‡Ø¯ÙÙˆÙ† Ø³ÙˆÙ†ÛŒ",
    "token_mapping": {{
        "headphones": "Ù‡Ø¯ÙÙˆÙ†",
        "Sony": "Ø³ÙˆÙ†ÛŒ"
    }},
    "intent": "find_high_quality",
    "price_sensitivity": 0.0,
    "quality_sensitivity": 1.0
}}

Input: "Ù„Ù¾ØªØ§Ù¾ Ø§ÛŒØ³ÙˆØ³ Ú¯ÛŒÙ…ÛŒÙ†Ú¯"
Output: {{
    "equip_prompt": "product_name: ASUS gaming laptop sort: relevance filter: brand=ASUS",
    "persian_full_query": "Ù„Ù¾ØªØ§Ù¾ Ø§ÛŒØ³ÙˆØ³ Ú¯ÛŒÙ…ÛŒÙ†Ú¯",
    "token_mapping": {{
        "laptop": "Ù„Ù¾ØªØ§Ù¾",
        "ASUS": "Ø§ÛŒØ³ÙˆØ³",
        "gaming": "Ú¯ÛŒÙ…ÛŒÙ†Ú¯"
    }},
    "intent": "find_by_feature",
    "price_sensitivity": 0.5,
    "quality_sensitivity": 0.5
}}

-----------------------------------------------
User Query: {query}

Output JSON:"""

        try:
            # Invoke LLM
            logger.debug("ğŸ’­ Invoking LLM for intent analysis...")
            response = self.llm.invoke(prompt)
            
            # Extract content from response
            response_text = ""
            if hasattr(response, 'content') and response.content:
                response_text = response.content.strip()
            elif hasattr(response, 'text') and response.text:
                response_text = response.text.strip()
            elif isinstance(response, dict):
                response_text = response.get('content', '') or response.get('text', '') or str(response)
            elif isinstance(response, str):
                response_text = response.strip()
            else:
                response_text = str(response).strip()
            
            logger.debug(f"ğŸ“„ LLM response: '{response_text[:300] if response_text else 'EMPTY'}'")
            
            if response_text:
                # Extract JSON from response
                json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response_text, re.DOTALL)
                if json_match:
                    try:
                        parsed = json.loads(json_match.group(0))
                        equip_prompt = parsed.get("equip_prompt", "")
                        persian_full_query = parsed.get("persian_full_query", query)
                        token_mapping = parsed.get("token_mapping", {})
                        intent = parsed.get("intent", intent)
                        price_sens = float(parsed.get("price_sensitivity", price_sens))
                        quality_sens = float(parsed.get("quality_sensitivity", quality_sens))
                        
                        logger.info(f"ğŸ¯ Parsed: equip_prompt='{equip_prompt[:50]}...', persian='{persian_full_query}'")
                    except json.JSONDecodeError as e:
                        logger.warning(f"âš ï¸ Failed to parse LLM JSON: {e}")
            
            logger.info(f"ğŸ¯ LLM extracted persian_full_query: '{persian_full_query}'")
            
        except Exception as e:
            logger.error(f"âŒ Error in LLM call: {str(e)}")
            # Fallback: create simple mapping
            equip_prompt = f"find {query}"
            persian_full_query = query
        
        # Call embedding server for category classification
        logger.debug(f"ğŸ·ï¸ Classifying categories for: '{persian_full_query}'")
        categories = await self.classify_categories(persian_full_query, top_k=3)
        categories_fa = [c.get("category", c) if isinstance(c, dict) else c for c in categories]
        
        # Clamp values
        price_sens = max(0.0, min(1.0, price_sens))
        quality_sens = max(0.0, min(1.0, quality_sens))
        
        # NOTE: Categories are NOT added to equip_prompt!
        # They will be added by dsl_processor_server after EQuIP generates the base DSL.
        # This prevents confusion in the EQuIP model with Persian text.
        
        result = {
            "equip_prompt": equip_prompt,
            "persian_full_query": persian_full_query,
            "token_mapping": token_mapping,
            "categories_fa": categories_fa,
            "intent": intent,
            "price_sensitivity": price_sens,
            "quality_sensitivity": quality_sens,
            "original_query": query
        }
        
        logger.info(f"âœ… Interpretation complete: {json.dumps(result, ensure_ascii=False)[:200]}...")
        return result


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Global Service Instance
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
interpret_service: InterpretService = None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MCP Server Setup
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
@asynccontextmanager
async def lifespan(server: FastMCP) -> AsyncIterator[dict]:
    """Initialize resources on startup."""
    global interpret_service
    logger.info(f"ğŸš€ Starting {SERVER_NAME} on port {SERVER_PORT}...")
    interpret_service = InterpretService()
    logger.info(f"âœ… {SERVER_NAME} ready!")
    yield {"interpret_service": interpret_service}
    logger.info(f"ğŸ‘‹ Shutting down {SERVER_NAME}...")


# Create MCP server
mcp = FastMCP(
    SERVER_NAME,
    lifespan=lifespan
)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MCP Tools
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
@mcp.tool()
async def interpret_query(query: str) -> str:
    """
    Analyze user shopping intent and prepare structured output for EQuIP DSL generation.
    
    Args:
        query: User's shopping query in natural language (Persian)
        
    Returns:
        JSON string with:
        - equip_prompt: English natural language query for EQuIP model
        - persian_full_query: Full Persian product description for BM25/semantic search
        - token_mapping: Dictionary mapping English words to Persian equivalents
        - categories_fa: List of Persian category names
        - intent: Shopping intent (find_cheapest, find_best_value, find_high_quality, compare, find_by_feature)
        - price_sensitivity: 0-1 (higher = more price-conscious)
        - quality_sensitivity: 0-1 (higher = more quality-focused)
        - original_query: Original user query
    """
    global interpret_service
    logger.debug(f"ğŸ“¥ interpret_query called with: '{query}'")
    
    try:
        result = await interpret_service.interpret(query)
        return json.dumps(result, ensure_ascii=False)
    except Exception as e:
        logger.error(f"âŒ Error interpreting query: {e}")
        return json.dumps({
            "error": str(e),
            "equip_prompt": f"find {query}",
            "persian_full_query": query,
            "token_mapping": {},
            "categories_fa": [],
            "intent": "find_best_value",
            "price_sensitivity": 0.5,
            "quality_sensitivity": 0.5,
            "original_query": query
        }, ensure_ascii=False)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Main Entry Point
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Configure mount path
mcp.settings.streamable_http_path = "/"

# Create ASGI app for uvicorn
app = mcp.streamable_http_app()

if __name__ == "__main__":
    import uvicorn
    
    logger.info(f"ğŸš€ Starting {SERVER_NAME} MCP Server...")
    logger.info(f"ğŸ“¡ Port: {SERVER_PORT}")
    logger.info(f"ğŸ”— Embedding Server: {EMBEDDING_SERVER_URL}")
    logger.info(f"ğŸ”§ Debug Mode: {DEBUG_MODE}")
    
    # Run with uvicorn
    uvicorn.run(app, host="0.0.0.0", port=SERVER_PORT)
